
@incollection{watson_good_2020,
	address = {Cham},
	title = {Good {Counterfactuals} and {Where} to {Find} {Them}: {A} {Case}-{Based} {Technique} for {Generating} {Counterfactuals} for {Explainable} {AI} ({XAI})},
	volume = {12311},
	isbn = {978-3-030-58341-5 978-3-030-58342-2},
	shorttitle = {Good {Counterfactuals} and {Where} to {Find} {Them}},
	url = {http://link.springer.com/10.1007/978-3-030-58342-2_11},
	abstract = {Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (i) technically, these counterfactual cases can be generated by permuting problem-features until a class-change is found, (ii) psychologically, they are much more causally informative than factual explanations, (iii) legally, they are GDPR-compliant. However, there are issues around the finding of “good” counterfactuals using current techniques (e.g. sparsity and plausibility). We show that many commonly-used datasets appear to have few “good” counterfactuals for explanation purposes. We propose a new case-based approach for generating counterfactuals, using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases, that were previously found wanting.},
	language = {en},
	urldate = {2023-12-11},
	booktitle = {Case-{Based} {Reasoning} {Research} and {Development}},
	publisher = {Springer International Publishing},
	author = {Keane, Mark T. and Smyth, Barry},
	editor = {Watson, Ian and Weber, Rosina},
	year = {2020},
	doi = {10.1007/978-3-030-58342-2_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {163--178},
	file = {Keane and Smyth - 2020 - Good Counterfactuals and Where to Find Them A Cas.pdf:C\:\\Users\\Celine\\Zotero\\storage\\E5WVAHHF\\Keane and Smyth - 2020 - Good Counterfactuals and Where to Find Them A Cas.pdf:application/pdf},
}

@article{akula_cocox_2020,
	title = {{CoCoX}: {Generating} {Conceptual} and {Counterfactual} {Explanations} via {Fault}-{Lines}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{CoCoX}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5643},
	doi = {10.1609/aaai.v34i03.5643},
	abstract = {We present CoCoX (short for Conceptual and Counterfactual Explanations), a model for explaining decisions made by a deep convolutional neural network (CNN). In Cognitive Psychology, the factors (or semantic-level features) that humans zoom in on when they imagine an alternative to a model prediction are often referred to as fault-lines. Motivated by this, our CoCoX model explains decisions made by a CNN using fault-lines. Specifically, given an input image I for which a CNN classification model M predicts class cpred, our fault-line based explanation identifies the minimal semantic-level features (e.g., stripes on zebra, pointed ears of dog), referred to as explainable concepts, that need to be added to or deleted from I in order to alter the classification category of I by M to another specified class calt. We argue that, due to the conceptual and counterfactual nature of fault-lines, our CoCoX explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex deep learning models. Extensive quantitative and qualitative experiments verify our hypotheses, showing that CoCoX significantly outperforms the state-of-the-art explainable AI models. Our implementation is available at https://github.com/arjunakula/CoCoX},
	number = {03},
	urldate = {2023-12-11},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Akula, Arjun and Wang, Shuai and Zhu, Song-Chun},
	month = apr,
	year = {2020},
	pages = {2594--2601},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\CV2V2A2P\\Akula et al. - 2020 - CoCoX Generating Conceptual and Counterfactual Ex.pdf:application/pdf},
}

@inproceedings{balloccu_post_2022,
	address = {Madrid Spain},
	title = {Post {Processing} {Recommender} {Systems} with {Knowledge} {Graphs} for {Recency}, {Popularity}, and {Diversity} of {Explanations}},
	isbn = {978-1-4503-8732-3},
	url = {https://dl.acm.org/doi/10.1145/3477495.3532041},
	doi = {10.1145/3477495.3532041},
	language = {en},
	urldate = {2023-12-12},
	booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Marras, Mirko},
	month = jul,
	year = {2022},
	pages = {646--656},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\5WC4GK63\\Balloccu et al. - 2022 - Post Processing Recommender Systems with Knowledge.pdf:application/pdf},
}

@article{xu_adversarial_2020,
	title = {Adversarial {Counterfactual} {Learning} and {Evaluation} for {Recommender} {System}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2012.02295},
	doi = {10.48550/ARXIV.2012.02295},
	abstract = {The feedback data of recommender systems are often subject to what was exposed to the users; however, most learning and evaluation methods do not account for the underlying exposure mechanism. We first show in theory that applying supervised learning to detect user preferences may end up with inconsistent results in the absence of exposure information. The counterfactual propensity-weighting approach from causal inference can account for the exposure mechanism; nevertheless, the partial-observation nature of the feedback data can cause identifiability issues. We propose a principled solution by introducing a minimax empirical risk formulation. We show that the relaxation of the dual problem can be converted to an adversarial game between two recommendation models, where the opponent of the candidate model characterizes the underlying exposure mechanism. We provide learning bounds and conduct extensive simulation studies to illustrate and justify the proposed approach over a broad range of recommendation settings, which shed insights on the various benefits of the proposed approach.},
	urldate = {2023-12-12},
	author = {Xu, Da and Ruan, Chuanwei and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@inproceedings{xian_cafe_2020,
	address = {Virtual Event Ireland},
	title = {{CAFE}: {Coarse}-to-{Fine} {Neural} {Symbolic} {Reasoning} for {Explainable} {Recommendation}},
	isbn = {978-1-4503-6859-9},
	shorttitle = {{CAFE}},
	url = {https://dl.acm.org/doi/10.1145/3340531.3412038},
	doi = {10.1145/3340531.3412038},
	language = {en},
	urldate = {2023-12-17},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Xian, Yikun and Fu, Zuohui and Zhao, Handong and Ge, Yingqiang and Chen, Xu and Huang, Qiaoying and Geng, Shijie and Qin, Zhou and De Melo, Gerard and Muthukrishnan, S. and Zhang, Yongfeng},
	month = oct,
	year = {2020},
	pages = {1645--1654},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\2BMXT2LQ\\Xian et al. - 2020 - CAFE Coarse-to-Fine Neural Symbolic Reasoning for.pdf:application/pdf},
}

@article{jaimini_causalkg_2022,
	title = {{CausalKG}: {Causal} {Knowledge} {Graph} {Explainability} {Using} {Interventional} and {Counterfactual} {Reasoning}},
	volume = {26},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1089-7801, 1941-0131},
	shorttitle = {{CausalKG}},
	url = {https://ieeexplore.ieee.org/document/9706608/},
	doi = {10.1109/MIC.2021.3133551},
	number = {1},
	urldate = {2024-04-15},
	journal = {IEEE Internet Computing},
	author = {Jaimini, Utkarshani and Sheth, Amit},
	month = jan,
	year = {2022},
	pages = {43--50},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\7VYGAGH2\\Jaimini and Sheth - 2022 - CausalKG Causal Knowledge Graph Explainability Us.pdf:application/pdf},
}

@article{zellinger_counterfactual_2024,
	title = {Counterfactual {Reasoning} with {Knowledge} {Graph} {Embeddings}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2403.06936},
	doi = {10.48550/ARXIV.2403.06936},
	abstract = {Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns. An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules. In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention. In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning.},
	urldate = {2024-04-15},
	author = {Zellinger, Lena and Stephan, Andreas and Roth, Benjamin},
	year = {2024},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	annote = {Other
Accepted to EACL 2024},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\YD4NR66Y\\Zellinger et al. - 2024 - Counterfactual Reasoning with Knowledge Graph Embe.pdf:application/pdf},
}

@article{chiappa_path-specific_2019,
	title = {Path-{Specific} {Counterfactual} {Fairness}},
	volume = {33},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4777},
	doi = {10.1609/aaai.v33i01.33017801},
	abstract = {We consider the problem of learning fair decision systems from data in which a sensitive attribute might affect the decision along both fair and unfair pathways. We introduce a counterfactual approach to disregard effects along unfair pathways that does not incur in the same loss of individual-specific information as previous approaches. Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. We leverage recent developments in deep learning and approximate inference to develop a VAE-type method that is widely applicable to complex nonlinear models.},
	number = {01},
	urldate = {2024-04-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chiappa, Silvia},
	month = jul,
	year = {2019},
	pages = {7801--7808},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\3BQAMRUZ\\Chiappa - 2019 - Path-Specific Counterfactual Fairness.pdf:application/pdf},
}

@inproceedings{liu_practical_2022,
	address = {Washington DC USA},
	title = {Practical {Counterfactual} {Policy} {Learning} for {Top}-{K} {Recommendations}},
	isbn = {978-1-4503-9385-0},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539295},
	doi = {10.1145/3534678.3539295},
	language = {en},
	urldate = {2024-04-16},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Liu, Yaxu and Yen, Jui-Nan and Yuan, Bowen and Shi, Rundong and Yan, Peng and Lin, Chih-Jen},
	month = aug,
	year = {2022},
	pages = {1141--1151},
	file = {Liu et al. - 2022 - Practical Counterfactual Policy Learning for Top-K.pdf:C\:\\Users\\Celine\\Zotero\\storage\\XH43KA7J\\Liu et al. - 2022 - Practical Counterfactual Policy Learning for Top-K.pdf:application/pdf},
}

@inproceedings{tan_counterfactual_2021,
	address = {Virtual Event Queensland Australia},
	title = {Counterfactual {Explainable} {Recommendation}},
	isbn = {978-1-4503-8446-9},
	url = {https://dl.acm.org/doi/10.1145/3459637.3482420},
	doi = {10.1145/3459637.3482420},
	language = {en},
	urldate = {2024-04-16},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Tan, Juntao and Xu, Shuyuan and Ge, Yingqiang and Li, Yunqi and Chen, Xu and Zhang, Yongfeng},
	month = oct,
	year = {2021},
	pages = {1784--1793},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\4B33ZZHV\\Tan et al. - 2021 - Counterfactual Explainable Recommendation.pdf:application/pdf},
}

@article{yang_top-n_2021,
	title = {Top-{N} {Recommendation} with {Counterfactual} {User} {Preference} {Simulation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2109.02444},
	doi = {10.48550/ARXIV.2109.02444},
	abstract = {Top-N recommendation, which aims to learn user ranking-based preference, has long been a fundamental problem in a wide range of applications. Traditional models usually motivate themselves by designing complex or tailored architectures based on different assumptions. However, the training data of recommender system can be extremely sparse and imbalanced, which poses great challenges for boosting the recommendation performance. To alleviate this problem, in this paper, we propose to reformulate the recommendation task within the causal inference framework, which enables us to counterfactually simulate user ranking-based preferences to handle the data scarce problem. The core of our model lies in the counterfactual question: "what would be the user's decision if the recommended items had been different?". To answer this question, we firstly formulate the recommendation process with a series of structural equation models (SEMs), whose parameters are optimized based on the observed data. Then, we actively indicate many recommendation lists (called intervention in the causal inference terminology) which are not recorded in the dataset, and simulate user feedback according to the learned SEMs for generating new training samples. Instead of randomly intervening on the recommendation list, we design a learning-based method to discover more informative training samples. Considering that the learned SEMs can be not perfect, we, at last, theoretically analyze the relation between the number of generated samples and the model prediction error, based on which a heuristic method is designed to control the negative effect brought by the prediction error. Extensive experiments are conducted based on both synthetic and real-world datasets to demonstrate the effectiveness of our framework.},
	urldate = {2024-04-16},
	author = {Yang, Mengyue and Dai, Quanyu and Dong, Zhenhua and Chen, Xu and He, Xiuqiang and Wang, Jun},
	year = {2021},
	note = {Publisher: [object Object]
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Information Retrieval (cs.IR)},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\YFDFGTL7\\Yang et al. - 2021 - Top-N Recommendation with Counterfactual User Pref.pdf:application/pdf},
}

@inproceedings{wei_model-agnostic_2021,
	address = {Virtual Event Singapore},
	title = {Model-{Agnostic} {Counterfactual} {Reasoning} for {Eliminating} {Popularity} {Bias} in {Recommender} {System}},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467289},
	doi = {10.1145/3447548.3467289},
	language = {en},
	urldate = {2024-04-16},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Wei, Tianxin and Feng, Fuli and Chen, Jiawei and Wu, Ziwei and Yi, Jinfeng and He, Xiangnan},
	month = aug,
	year = {2021},
	pages = {1791--1800},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\NL6UHIYH\\Wei et al. - 2021 - Model-Agnostic Counterfactual Reasoning for Elimin.pdf:application/pdf},
}

@inproceedings{ghazimatin_prince_2020,
	address = {Houston TX USA},
	title = {{PRINCE}: {Provider}-side {Interpretability} with {Counterfactual} {Explanations} in {Recommender} {Systems}},
	isbn = {978-1-4503-6822-3},
	shorttitle = {{PRINCE}},
	url = {https://dl.acm.org/doi/10.1145/3336191.3371824},
	doi = {10.1145/3336191.3371824},
	language = {en},
	urldate = {2024-04-16},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ghazimatin, Azin and Balalau, Oana and Saha Roy, Rishiraj and Weikum, Gerhard},
	month = jan,
	year = {2020},
	pages = {196--204},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\ZNU5FLZZ\\Ghazimatin et al. - 2020 - PRINCE Provider-side Interpretability with Counte.pdf:application/pdf},
}

@inproceedings{chen_dark_2023,
	address = {Taipei Taiwan},
	title = {The {Dark} {Side} of {Explanations}: {Poisoning} {Recommender} {Systems} with {Counterfactual} {Examples}},
	isbn = {978-1-4503-9408-6},
	shorttitle = {The {Dark} {Side} of {Explanations}},
	url = {https://dl.acm.org/doi/10.1145/3539618.3592070},
	doi = {10.1145/3539618.3592070},
	language = {en},
	urldate = {2024-04-17},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Chen, Ziheng and Silvestri, Fabrizio and Wang, Jia and Zhang, Yongfeng and Tolomei, Gabriele},
	month = jul,
	year = {2023},
	pages = {2426--2430},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\NENJBSSI\\Chen et al. - 2023 - The Dark Side of Explanations Poisoning Recommend.pdf:application/pdf},
}

@inproceedings{tran_counterfactual_2021,
	address = {Virtual Event Canada},
	title = {Counterfactual {Explanations} for {Neural} {Recommenders}},
	isbn = {978-1-4503-8037-9},
	url = {https://dl.acm.org/doi/10.1145/3404835.3463005},
	doi = {10.1145/3404835.3463005},
	language = {en},
	urldate = {2024-04-17},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Tran, Khanh Hiep and Ghazimatin, Azin and Saha Roy, Rishiraj},
	month = jul,
	year = {2021},
	pages = {1627--1631},
	annote = {1
},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\CDQRSJ7P\\Tran et al. - 2021 - Counterfactual Explanations for Neural Recommender.pdf:application/pdf},
}

@article{wang_incorporating_2021,
	title = {Incorporating prior knowledge from counterfactuals into knowledge graph reasoning},
	volume = {223},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121002987},
	doi = {10.1016/j.knosys.2021.107035},
	language = {en},
	urldate = {2024-04-17},
	journal = {Knowledge-Based Systems},
	author = {Wang, Zikang and Li, Linjing and Zeng, Daniel and Wu, Xiaofei},
	month = jul,
	year = {2021},
	pages = {107035},
	annote = {What they did in short:In this study, the authors developed a method for enhancing multi-hop reasoning in knowledge graphs by incorporating prior knowledge derived from counterfactual analysis. For each query, they initially identify possible paths within the knowledge graph that connect a head entity to potential tail entities. They then generate counterfactual paths by altering one relation at a time, analyzing how such changes impact the semantic outcome and consistency with the original query. This analysis helps assign statistical weights to each relation, indicating their importance for specific queries. These weights are integrated as prior knowledge into a reinforcement learning model, improving the model's decision-making process and enabling more accurate and efficient reasoning as it trains to predict tail entities based on the weighted paths.
},
	file = {Wang et al. - 2021 - Incorporating prior knowledge from counterfactuals.pdf:C\:\\Users\\Celine\\Zotero\\storage\\WCFXMK6V\\Wang et al. - 2021 - Incorporating prior knowledge from counterfactuals.pdf:application/pdf},
}

@inproceedings{guo_towards_2023,
	address = {Birmingham United Kingdom},
	title = {Towards {Fair} {Graph} {Neural} {Networks} via {Graph} {Counterfactual}},
	isbn = {9798400701245},
	url = {https://dl.acm.org/doi/10.1145/3583780.3615092},
	doi = {10.1145/3583780.3615092},
	language = {en},
	urldate = {2024-04-17},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Guo, Zhimeng and Li, Jialiang and Xiao, Teng and Ma, Yao and Wang, Suhang},
	month = oct,
	year = {2023},
	pages = {669--678},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\Z8E6ISDA\\Guo et al. - 2023 - Towards Fair Graph Neural Networks via Graph Count.pdf:application/pdf},
}

@inproceedings{tan_learning_2022,
	address = {Virtual Event, Lyon France},
	title = {Learning and {Evaluating} {Graph} {Neural} {Network} {Explanations} based on {Counterfactual} and {Factual} {Reasoning}},
	isbn = {978-1-4503-9096-5},
	url = {https://dl.acm.org/doi/10.1145/3485447.3511948},
	doi = {10.1145/3485447.3511948},
	language = {en},
	urldate = {2024-04-17},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {ACM},
	author = {Tan, Juntao and Geng, Shijie and Fu, Zuohui and Ge, Yingqiang and Xu, Shuyuan and Li, Yunqi and Zhang, Yongfeng},
	month = apr,
	year = {2022},
	pages = {1018--1027},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\HND7QZP4\\Tan et al. - 2022 - Learning and Evaluating Graph Neural Network Expla.pdf:application/pdf},
}

@article{lucic_cf-gnnexplainer_2021,
	title = {{CF}-{GNNExplainer}: {Counterfactual} {Explanations} for {Graph} {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{CF}-{GNNExplainer}},
	url = {https://arxiv.org/abs/2102.03322},
	doi = {10.48550/ARXIV.2102.03322},
	abstract = {Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94{\textbackslash}\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.},
	urldate = {2024-04-17},
	author = {Lucic, Ana and ter Hoeve, Maartje and Tolomei, Gabriele and de Rijke, Maarten and Silvestri, Fabrizio},
	year = {2021},
	note = {Publisher: [object Object]
Version Number: 4},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	annote = {code: https://github.com/a-lucic/cf-gnnexplainer
},
	annote = {Other
Accepted to AISTATS 2022},
	annote = {What they did in short:
The researchers developed CF-GNNExplainer, a method for generating counterfactual explanations for Graph Neural Networks (GNNs) by perturbing the adjacency matrix of a graph. They use a modified version of a one-layer Graph Convolutional Network (GCN), where they iteratively adjust a binary perturbation matrix to sparsify the graph, minimizing a loss function that combines prediction divergence and perturbation minimality. This approach helps to identify the minimal changes in the graph structure that lead to different model predictions, enhancing the interpretability of GNNs.
},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\LHWRFQ22\\Lucic et al. - 2021 - CF-GNNExplainer Counterfactual Explanations for G.pdf:application/pdf},
}

@article{ma_clear_2022,
	title = {{CLEAR}: {Generative} {Counterfactual} {Explanations} on {Graphs}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{CLEAR}},
	url = {https://arxiv.org/abs/2210.08443},
	doi = {10.48550/ARXIV.2210.08443},
	abstract = {Counterfactual explanations promote explainability in machine learning models by answering the question "how should an input instance be perturbed to obtain a desired predicted label?". The comparison of this instance before and after perturbation can enhance human interpretation. Most existing studies on counterfactual explanations are limited in tabular data or image data. In this work, we study the problem of counterfactual explanation generation on graphs. A few studies have explored counterfactual explanations on graphs, but many challenges of this problem are still not well-addressed: 1) optimizing in the discrete and disorganized space of graphs; 2) generalizing on unseen graphs; and 3) maintaining the causality in the generated counterfactuals without prior knowledge of the causal model. To tackle these challenges, we propose a novel framework CLEAR which aims to generate counterfactual explanations on graphs for graph-level prediction models. Specifically, CLEAR leverages a graph variational autoencoder based mechanism to facilitate its optimization and generalization, and promotes causality by leveraging an auxiliary variable to better identify the underlying causal model. Extensive experiments on both synthetic and real-world graphs validate the superiority of CLEAR over the state-of-the-art methods in different aspects.},
	urldate = {2024-04-17},
	author = {Ma, Jing and Guo, Ruocheng and Mishra, Saumitra and Zhang, Aidong and Li, Jundong},
	year = {2022},
	note = {Publisher: [object Object]
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
	annote = {Other
18 pages, 9 figures},
	annote = {What they did in short:The authors developed CLEAR, a generative model-agnostic framework using a graph variational auto-encoder (VAE) to create counterfactual explanations for graph-based data. The model encodes a graph into a latent space, then decodes it to generate counterfactual versions that aim for a specific, different prediction outcome, while incorporating causality through the use of an auxiliary variable to improve the realism and feasibility of the generated counterfactuals.
},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\7XMGYK7W\\Ma et al. - 2022 - CLEAR Generative Counterfactual Explanations on G.pdf:application/pdf},
}

@article{wei_causal_2023,
	title = {Causal {Inference} for {Knowledge} {Graph} {Based} {Recommendation}},
	volume = {35},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9996555/},
	doi = {10.1109/TKDE.2022.3231352},
	number = {11},
	urldate = {2024-04-20},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wei, Yinwei and Wang, Xiang and Nie, Liqiang and Li, Shaoyu and Wang, Dingxian and Chua, Tat-Seng},
	month = nov,
	year = {2023},
	pages = {11153--11164},
	annote = {What They did in short:The KGCR model integrates Graph Convolutional Networks (GCNs) and causal inference, utilizing TransE embeddings to initialize the representations of entities within the knowledge graph. These embeddings capture the structural and relational dynamics of the KG, where the TransE model interprets relationships as translations in the embedding space, thus h + r ≈ t for head h, relation r, and tail t. Counterfactual scenarios are generated by applying do-calculus to intervene on the attribute exposure paths in the causal graph, recalculating similarity scores under hypothetical conditions where attribute influences are isolated from structural confounders. This method allows for unbiased estimations of user-item interactions, enhancing recommendation accuracy by removing spurious correlations.
},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\CFTJYLCR\\Wei et al. - 2023 - Causal Inference for Knowledge Graph Based Recomme.pdf:application/pdf},
}

@article{wang_explainable_2019,
	title = {Explainable {Reasoning} over {Knowledge} {Graphs} for {Recommendation}},
	volume = {33},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4470},
	doi = {10.1609/aaai.v33i01.33015329},
	abstract = {Incorporating knowledge graph into recommender systems has attracted increasing attention in recent years. By exploring the interlinks within a knowledge graph, the connectivity between users and items can be discovered as paths, which provide rich and complementary information to user-item interactions. Such connectivity not only reveals the semantics of entities and relations, but also helps to comprehend a user’s interest. However, existing efforts have not fully explored this connectivity to infer user preferences, especially in terms of modeling the sequential dependencies within and holistic semantics of a path.In this paper, we contribute a new model named Knowledgeaware Path Recurrent Network (KPRN) to exploit knowledge graph for recommendation. KPRN can generate path representations by composing the semantics of both entities and relations. By leveraging the sequential dependencies within a path, we allow effective reasoning on paths to infer the underlying rationale of a user-item interaction. Furthermore, we design a new weighted pooling operation to discriminate the strengths of different paths in connecting a user with an item, endowing our model with a certain level of explainability. We conduct extensive experiments on two datasets about movie and music, demonstrating significant improvements over state-of-the-art solutions Collaborative Knowledge Base Embedding and Neural Factorization Machine.},
	number = {01},
	urldate = {2024-04-20},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Xiang and Wang, Dingxian and Xu, Canran and He, Xiangnan and Cao, Yixin and Chua, Tat-Seng},
	month = jul,
	year = {2019},
	pages = {5329--5336},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\CKCXZV5V\\Wang et al. - 2019 - Explainable Reasoning over Knowledge Graphs for Re.pdf:application/pdf},
}

@article{ai_learning_2018,
	title = {Learning {Heterogeneous} {Knowledge} {Base} {Embeddings} for {Explainable} {Recommendation}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1999-4893},
	url = {http://www.mdpi.com/1999-4893/11/9/137},
	doi = {10.3390/a11090137},
	abstract = {Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms—especially the collaborative filtering (CF)- based approaches with shallow or deep models—usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users’ historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines.},
	language = {en},
	number = {9},
	urldate = {2024-04-25},
	journal = {Algorithms},
	author = {Ai, Qingyao and Azizi, Vahid and Chen, Xu and Zhang, Yongfeng},
	month = sep,
	year = {2018},
	pages = {137},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\HIFY7I9P\\Ai et al. - 2018 - Learning Heterogeneous Knowledge Base Embeddings f.pdf:application/pdf},
}

@inproceedings{geng_path_2022,
	address = {Virtual Event, Lyon France},
	title = {Path {Language} {Modeling} over {Knowledge} {Graphsfor} {Explainable} {Recommendation}},
	isbn = {978-1-4503-9096-5},
	url = {https://dl.acm.org/doi/10.1145/3485447.3511937},
	doi = {10.1145/3485447.3511937},
	language = {en},
	urldate = {2024-05-06},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {ACM},
	author = {Geng, Shijie and Fu, Zuohui and Tan, Juntao and Ge, Yingqiang and De Melo, Gerard and Zhang, Yongfeng},
	month = apr,
	year = {2022},
	pages = {946--955},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\A2R7NW8H\\Geng et al. - 2022 - Path Language Modeling over Knowledge Graphsfor Ex.pdf:application/pdf},
}

@article{medda_gnnuers_2024,
	title = {{GNNUERS}: {Fairness} {Explanation} in {GNNs} for {Recommendation} via {Counterfactual} {Reasoning}},
	issn = {2157-6904, 2157-6912},
	shorttitle = {{GNNUERS}},
	url = {https://dl.acm.org/doi/10.1145/3655631},
	doi = {10.1145/3655631},
	abstract = {Nowadays, research into personalization has been focusing on explainability and fairness. Several approaches proposed in recent works are able to explain individual recommendations in a post-hoc manner or by explanation paths. However, explainability techniques applied to unfairness in recommendation have been limited to finding user/item features mostly related to biased recommendations. In this paper, we devised a novel algorithm that leverages counterfactuality methods to discover user unfairness explanations in the form of user-item interactions. In our counterfactual framework, interactions are represented as edges in a bipartite graph, with users and items as nodes. Our bipartite graph explainer perturbs the topological structure to find an altered version that minimizes the disparity in utility between the protected and unprotected demographic groups. Experiments on four real-world graphs coming from various domains showed that our method can systematically explain user unfairness on three state-of-the-art GNN-based recommendation models. Moreover, an empirical evaluation of the perturbed network uncovered relevant patterns that justify the nature of the unfairness discovered by the generated explanations. The source code and the preprocessed data sets are available at https://github.com/jackmedda/RS-BGExplainer.},
	language = {en},
	urldate = {2024-05-07},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Medda, Giacomo and Fabbri, Francesco and Marras, Mirko and Boratto, Ludovico and Fenu, Gianni},
	month = apr,
	year = {2024},
	pages = {3655631},
	file = {Full Text:C\:\\Users\\Celine\\Zotero\\storage\\P7ERWIY9\\Medda et al. - 2024 - GNNUERS Fairness Explanation in GNNs for Recommen.pdf:application/pdf},
}

@article{wang_reinforced_2024,
	title = {Reinforced {Path} {Reasoning} for {Counterfactual} {Explainable} {Recommendation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/10399934/},
	doi = {10.1109/TKDE.2024.3354077},
	urldate = {2024-05-07},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wang, Xiangmeng and Li, Qian and Yu, Dianer and Li, Qing and Xu, Guandong},
	year = {2024},
	pages = {1--17},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\Y4DVTXI2\\Wang et al. - 2024 - Reinforced Path Reasoning for Counterfactual Expla.pdf:application/pdf},
}

@inproceedings{noauthor_explainable_nodate,
	title = {Explainable {GNN}-based models over knowledge graphs},
}

@inproceedings{xian_reinforcement_2019,
	address = {Paris France},
	title = {Reinforcement {Knowledge} {Graph} {Reasoning} for {Explainable} {Recommendation}},
	isbn = {978-1-4503-6172-9},
	url = {https://dl.acm.org/doi/10.1145/3331184.3331203},
	doi = {10.1145/3331184.3331203},
	language = {en},
	urldate = {2024-05-17},
	booktitle = {Proceedings of the 42nd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Xian, Yikun and Fu, Zuohui and Muthukrishnan, S. and De Melo, Gerard and Zhang, Yongfeng},
	month = jul,
	year = {2019},
	pages = {285--294},
	file = {Submitted Version:C\:\\Users\\Celine\\Zotero\\storage\\56IHWS7C\\Xian et al. - 2019 - Reinforcement Knowledge Graph Reasoning for Explai.pdf:application/pdf},
}
